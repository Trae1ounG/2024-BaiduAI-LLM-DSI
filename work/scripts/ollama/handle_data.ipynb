{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T07:29:36.246322Z",
     "iopub.status.busy": "2024-08-18T07:29:36.245690Z",
     "iopub.status.idle": "2024-08-18T07:30:05.320689Z",
     "shell.execute_reply": "2024-08-18T07:30:05.319568Z",
     "shell.execute_reply.started": "2024-08-18T07:29:36.246289Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusai-2024-cti-bigmodel-retrieval-data/\r\n",
      "fusai-2024-cti-bigmodel-retrieval-data/train_data.csv\r\n",
      "fusai-2024-cti-bigmodel-retrieval-data/ad_landingpage.txt\r\n",
      "fusai-2024-cti-bigmodel-retrieval-data/ad_core_term.txt\r\n",
      "fusai-2024-cti-bigmodel-retrieval-data/dev_data.csv\r\n",
      "ad_core_term.txt    dev_data.csv\t fusai_train_data.json\r\n",
      "ad_landingpage.txt  fusai_dev_data.json  train_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "!mkdir /home/aistudio/data/data274453/fusai-2024-cti-bigmodel-retrieval-data\n",
    "!tar  -zxvf /home/aistudio/data/data274453/fusai-2024-cti-bigmodel-retrieval-data.tar.gz -C /home/aistudio/data/data274453/fusai-2024-cti-bigmodel-retrieval-data\n",
    "os.chdir(\"/home/aistudio/data/data274453/fusai-2024-cti-bigmodel-retrieval-data\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T07:17:42.215861Z",
     "iopub.status.busy": "2024-08-18T07:17:42.215314Z",
     "iopub.status.idle": "2024-08-18T07:18:07.799099Z",
     "shell.execute_reply": "2024-08-18T07:18:07.798157Z",
     "shell.execute_reply.started": "2024-08-18T07:17:42.215831Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#将训练和验证数据写入为json格式\n",
    "with open('dev_data.csv', 'r') as f:\n",
    "    with open('fusai_dev_data.json', 'w', buffering=1024*1024) as w:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            src, tgt, click = line.split('\\t')\n",
    "            data = {\"src\":src, \"tgt\":tgt, \"click\":click}\n",
    "            w.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "with open('train_data.csv', 'r') as f:\n",
    "    with open('fusai_train_data.json', 'w', buffering=1024*1024) as w:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            src, tgt, click = line.split('\\t')\n",
    "            data = {\"src\":src, \"tgt\":tgt, \"click\":click}\n",
    "            w.write(json.dumps(data, ensure_ascii=False) + \"\\n\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.567208496080635\r\n",
      "50.74696851851643\r\n",
      "888427\r\n",
      "45084977\r\n"
     ]
    }
   ],
   "source": [
    "#统计落地页数据信息\n",
    "import numpy as np\n",
    "with open('ad_landingpage.txt', 'r', buffering=1024*1024) as f:\n",
    "    lines = f.readlines()\n",
    "    a = [len(line.split(\"<block>\")) for line in lines]\n",
    "    cnt = 0\n",
    "    total_len = 0\n",
    "    for line in lines:\n",
    "        src = line.split('\\t')[1]\n",
    "        for i in src.split(\"<block>\"):\n",
    "            cnt += 1\n",
    "            total_len += len(i)\n",
    "    print(total_len / cnt)\n",
    "    print(np.mean(a))\n",
    "    print(len(lines))\n",
    "    print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.776372172390078\r\n",
      "888427\r\n",
      "2466604\r\n"
     ]
    }
   ],
   "source": [
    "#统计核心词数据信息\n",
    "import numpy as np\n",
    "with open('ad_core_term.txt', 'r', buffering=1024*1024) as f:\n",
    "    lines = f.readlines()\n",
    "    a = [len(line.split(\"<sep>\")) for line in lines]\n",
    "    print(np.mean(a))\n",
    "    print(len(lines))\n",
    "    print(sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#清理ollama生成的数据\n",
    "import jieba\n",
    "import json\n",
    "from tqdm import *\n",
    "with open(\"./fusai_train_data.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]\n",
    "d_key = dict()\n",
    "for d in data:\n",
    "#清洗思路为：要求每个广告词的长度在4-20之间，并且包含训练数据中使用jieba切分后的任意一个词即可\n",
    "    keywords = set(jieba.cut(d['src']))\n",
    "    d_key[int(d['tgt'])] = keywords\n",
    "\n",
    "#此处为未清洗前的数据\n",
    "#可以使用/home/aistudio/work/scripts/ollama/test_ollama_*得到\n",
    "with open(\"/home/aistudio/work/scripts/ollama/train_indexing_qwen_merge_cleaned.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    to_mine = [json.loads(line) for line in lines]\n",
    "\n",
    "def contains_keyword(s, keywords):\n",
    "    return any(keyword in s for keyword in keywords)\n",
    "\n",
    "#清洗数据：\n",
    "mined_data = []\n",
    "for item in to_mine:\n",
    "    split_data = item['src'].split(\";\")\n",
    "    print(split_data)\n",
    "    if int(item['tgt']) not in d_key:\n",
    "        print(  f\"不存在{item['tgt']}\")\n",
    "        mined_data.extend([{'tgt': item['tgt'], 'src': s} for s in split_data])\n",
    "        continue\n",
    "    print(d_key[int(item['tgt'])])\n",
    "    \n",
    "    filtered_data = [s for s in split_data if 4 < len(s) < 20 and contains_keyword(s, d_key[int(item['tgt'])])]\n",
    "    filtered_data = set(filtered_data)\n",
    "    # Add the data to the mined data\n",
    "    mined_data.extend([{'tgt': item['tgt'], 'src': s} for s in filtered_data])\n",
    "\n",
    "# Print the mined data\n",
    "with open('/home/aistudio/work/data/fusai_cleaned_ollama_1.json', \"w\", buffering=1024*1024) as w:\n",
    "    for item in tqdm(mined_data):\n",
    "        # print(item)\n",
    "        w.write(json.dumps(item,ensure_ascii=False) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#合并数据\n",
    "with open(\"/home/aistudio/work/data/fusai_cleaned_ollama_1.json\", \"r\") as f:\n",
    "    with open(\"./fusai_train_data.json\", \"r\") as f1:\n",
    "       with open(\"/home/aistudio/work/data/fusai_query.json\", \"w\", buffering=1024*1024) as w:\n",
    "        for line in f:\n",
    "            w.write(line)\n",
    "        for line in f1:\n",
    "            w.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T07:18:57.608126Z",
     "iopub.status.busy": "2024-08-18T07:18:57.607621Z",
     "iopub.status.idle": "2024-08-18T07:19:44.245681Z",
     "shell.execute_reply": "2024-08-18T07:19:44.244768Z",
     "shell.execute_reply.started": "2024-08-18T07:18:57.608095Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437851"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#统计训练数据每个tgt对应src和click的数量\n",
    "d_query = dict()\n",
    "d_max_click = dict()\n",
    "with open('./fusai_train_data.json', 'r') as w:\n",
    "    for line in w:\n",
    "        data = json.loads(line)\n",
    "        src, tgt, click = data['src'], int(data['tgt']), float(data['click'])\n",
    "        if tgt not in d_query:\n",
    "            d_query[tgt] = []\n",
    "        d_query[tgt].append([src,click])\n",
    "        if tgt not in d_max_click:\n",
    "            d_max_click[tgt] = click\n",
    "        else:\n",
    "            d_max_click[tgt] = max(d_max_click[tgt],click)\n",
    "# click的分位数 (25%, 50%, 90%): [1. 1. 4.]\n",
    "# 每个tgt包含的[src, click]对个数的分位数 (10%, 50%, 75%): [4.4 6.  8.5]\n",
    "use_tgts = []\n",
    "for tgt in range(1,888427 + 1):\n",
    "    if tgt not in d_query or d_max_click[tgt] >= 4.0 or len(d_query) < 5:\n",
    "        use_tgts.append(tgt)\n",
    "len(use_tgts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T07:20:35.938186Z",
     "iopub.status.busy": "2024-08-18T07:20:35.937533Z",
     "iopub.status.idle": "2024-08-18T07:20:46.675161Z",
     "shell.execute_reply": "2024-08-18T07:20:46.674156Z",
     "shell.execute_reply.started": "2024-08-18T07:20:35.938152Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\r\n",
      "2.5756564\r\n",
      "click的分位数 (25%, 50%, 75%): [1. 1. 4.]\r\n",
      "每个tgt包含的[src, click]对个数的分位数 (25%, 50%, 75%): [4.4 6.  8.5]\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 提取所有click的值\n",
    "clicks = [click for src_click_pairs in d_query.values() for src, click in src_click_pairs]\n",
    "# print(clicks)\n",
    "# 计算click的中位数\n",
    "# 计算click的25%、50%（中位数）和75%分位数\n",
    "click_percentiles = np.percentile(clicks, [25, 50, 90])\n",
    "\n",
    "# 计算每个tgt包含的[src, click]对个数\n",
    "tgt_counts = [len(src_click_pairs) for src_click_pairs in data.values()]\n",
    "\n",
    "# 计算每个tgt包含的[src, click]对个数的25%、50%（中位数）和75%分位数\n",
    "tgt_count_percentiles = np.percentile(tgt_counts, [10, 50, 75])\n",
    "print(np.max(clicks))\n",
    "print(np.mean(clicks))\n",
    "print(\"click的分位数 (25%, 50%, 75%):\", click_percentiles)\n",
    "print(\"每个tgt包含的[src, click]对个数的分位数 (25%, 50%, 75%):\", tgt_count_percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#对ollama生成的每一项进行进一步清洗\n",
    "#注意：该步骤需要迭代重复两次\n",
    "import json\n",
    "with open(\"/home/aistudio/work/data/fusai_cleaned_ollama_1.json\", \"r\") as f:\n",
    "    with open(\"/home/aistudio/work/data/fusai_cleaned_ollama_1_tmp.json\", \"w\") as w:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            src = data['src']\n",
    "            if \"；\" in src:\n",
    "                srcs = src.split(\"；\")\n",
    "                for s in srcs:\n",
    "                    src = s\n",
    "                    if \"\\n\" in src:\n",
    "                        src = src.replace(\"\\n\",\"\")\n",
    "                    if \"1. \" in src:\n",
    "                        src = src.replace(\"1. \",\"\")\n",
    "                    if \"2. \" in src:\n",
    "                        src = src.replace(\"2. \",\"\")\n",
    "                    if \"3. \" in src:\n",
    "                        src = src.replace(\"3. \",\"\")\n",
    "                    src = src.strip()\n",
    "                    if len(src) == 0 or len(src) > 20:\n",
    "                        continue\n",
    "                    w.write(json.dumps({\"src\":s,\"tgt\":data[\"tgt\"]}, ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "            if \"\\n\" in src:\n",
    "                src = src.replace(\"\\n\",\"\")\n",
    "            if \"|\" in src:\n",
    "                src = src.replace(\"|\",\"\")\n",
    "            if \"1. \" in src:\n",
    "                src = src.replace(\"1. \",\"\")\n",
    "            if \"2. \" in src:\n",
    "                src = src.replace(\"2. \",\"\")\n",
    "            if \"3. \" in src:\n",
    "                src = src.replace(\"3. \",\"\")\n",
    "            if \"、\" in src:\n",
    "                src = src.replace(\"、\",\"、\")\n",
    "            if \"\\n\" in src:\n",
    "                src = src.replace(\"\\n\",\"\")\n",
    "            if '\\\"' in src:\n",
    "                src = src.replace('\\\"',\"\")\n",
    "            if len(src.split(\" \")) == 3:\n",
    "                continue\n",
    "            src = src.strip()\n",
    "            if len(src) == 0 or len(src) > 20 or len(src) < 4:\n",
    "                continue\n",
    "            else:\n",
    "                w.write(json.dumps({\"src\":src,\"tgt\":data[\"tgt\"]}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#合并ollama数据\n",
    "with open(\"/home/aistudio/work/data/fusai_cleaned_ollama_all.json\", \"w\", buffering=1024 * 1024) as w:\n",
    "    with open(\"/home/aistudio/work/scripts/ollama/test_ollama_group_gemma2_data.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            w.write(line)\n",
    "    with open(\"/home/aistudio/work/scripts/ollama/test_ollama_group_all_data.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            w.write(line)\n",
    "    with open(\"/home/aistudio/work/scripts/ollama/test_ollama_group_qwen1.8_data.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            w.write(line)\n",
    "    with open(\"/home/aistudio/work/scripts/ollama/test_ollama_group_data.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            w.write(line)\n",
    "    with open(\"/home/aistudio/work/data/fusai_cleaned_ollama_1.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            w.write(line)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "fileId": "89487947-2031-40e0-a1ff-b9b637940ba1",
  "filePath": "/root/data/handle_data.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
